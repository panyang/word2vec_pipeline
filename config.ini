target_column  = abstract
_PARALLEL = True

[import_data]

    input_data_directories = datasets,
    output_data_directory  = data_import
    data_type = csv
    output_table = original

[phrase_identification]

    f_abbreviations = abbreviations.csv
    output_data_directory = data_import/abbreviations

[parse]

    output_table = parsed
    output_data_directory = data_parsed

    pipeline = dedash, titlecaps, replace_phrases, remove_parenthesis, replace_from_dictionary, token_replacement, decaps_text, pos_tokenizer

    [[replace_from_dictionary]]
        input_data_directory = w2v_pipeline/preprocessing/dictionaries
        f_dict = MeSH_two_word_lexicon.csv

    [[replace_phrases]]
        input_data_directory = data_import/abbreviations/
        f_abbreviations = abbreviations.csv

    [[pos_tokenizer]]
        POS_blacklist = connector, cardinal, pronoun, symbol, punctuation, modal_verb, adverb, verb, w_word

[embedding]

    input_data_directory  = data_parsed
    output_data_directory = data_embeddings
    embedding_commands    = w2v_embedding,
    #embedding_commands    = d2v_embedding,

    [[w2v_embedding]]      
        f_db = w2v.gensim
      	skip_gram = 0
      	hierarchical_softmax = 0
        epoch_n = 80
        window = 5
        negative = 5
        sample = 1e-5
        size = 300
        min_count = 10

    [[d2v_embedding]]      
        f_db = d2v.gensim
        epoch_n = 80
        window = 5
        negative = 5
        sample = 1e-5
        size = 300
        min_count = 10

[score]

    output_data_directory = data_document_scores

    mapreduce_commands    = term_document_frequency, term_frequency, document_log_probability, 
    globaldata_commands   = score_unique_TF, score_simple_TF, score_unique, score_simple, score_Z_weighted, score_locality_hash, reduced_representation

    #mapreduce_commands    = ,
    #globaldata_commands   = reduced_representation,
  
    input_file_whitelist = ,
    compute_reduced_representation = True

    [[negative_weights]]
        # Sample negative weights, adjust as needed
        understand = 0.15   
        scientific = 0.25

    [[reduced_representation]]
        n_components = 25
        rescored_command = unique_TF
	bais_strength = 15.0

    [[term_frequency]]
        f_db = TF.csv

    [[term_document_frequency]]   
        f_db = TDF.csv

    [[document_log_probability]]
      	f_partition_function = w2v_partition_function.h5
      	f_db = log_prob.h5
      	intra_document_cutoff = 0.10

    [[score_Z_weighted]]
      	kT = 1.5
      	threshold = 0.0

    [[score_simple]]
    [[score_unique]]
    [[score_simple_TF]]
    [[score_unique_TF]]
    [[score_locality_hash]]
        locality_n_bits = 12
        locality_alpha  = 0.00

    [[document_scores]]
        f_db  = document_scores.h5

[predict]
    categorical_columns = journal,

    n_estimators = 200
    cross_validation_folds = 12
  
    use_SMOTE = False
    use_reduced = True
    use_meta = True
  
    meta_methods = unique_TF, reduced_representation

[metacluster]

    score_method = unique_TF

    subcluster_m = 1000
    subcluster_kn = 10

    subcluster_pcut = 0.80
    subcluster_repeats = 1

    output_data_directory = data_clustering
    f_centroids = meta_cluster_centroids.h5


[cluster]

    predict_target_directory = data_import/
    score_method = unique_TF

    output_data_directory = data_clustering
    f_cluster = clustering.h5
    clustering_commands = spectral_clustering,

    [[spectral_clustering]]
        n_clusters = 4

    [[hdbscan_clustering]]
        min_cluster_size = 30

[postprocessing]

    output_data_directory = results
    master_columns = PMID, title, 

